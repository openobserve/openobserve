// Copyright 2025 OpenObserve Inc.
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Affero General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
//
// This program is distributed in the hope that it will be useful
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Affero General Public License for more details.
//
// You should have received a copy of the GNU Affero General Public License
// along with this program.  If not, see <http://www.gnu.org/licenses/>.

use std::{ops::Range, sync::Arc};

use bytes::buf::Buf;
use config::{get_config, is_local_disk_storage, meta::stream::FileMeta, metrics};
use datafusion::parquet::{data_type::AsBytes, file::metadata::ParquetMetaData};
use futures::{StreamExt, TryStreamExt};
use object_store::{GetRange, GetResult, ObjectMeta, ObjectStore, WriteMultipart, path::Path};
use once_cell::sync::Lazy;
use parquet::file::metadata::ParquetMetaDataReader;

pub mod local;
pub mod remote;

pub const CONCURRENT_REQUESTS: usize = 1000;

pub static DEFAULT: Lazy<Box<dyn ObjectStore>> = Lazy::new(default);
pub static LOCAL_WAL: Lazy<Box<dyn ObjectStore>> = Lazy::new(local_wal);

/// Returns the default object store based on the configuration.
/// If the local disk storage is enabled, it creates a local object store.
/// Otherwise, it creates a remote object store.
///
/// # Examples
///
/// ```
/// use infra::storage::default;
///
/// let object_store = default();
/// ```
fn default() -> Box<dyn ObjectStore> {
    if is_local_disk_storage() {
        std::fs::create_dir_all(&get_config().common.data_stream_dir)
            .expect("create stream data dir success");
        Box::<local::Local>::default()
    } else {
        Box::<remote::Remote>::default()
    }
}

fn local_wal() -> Box<dyn ObjectStore> {
    let cfg = get_config();
    std::fs::create_dir_all(&cfg.common.data_wal_dir).expect("create wal dir success");
    Box::new(local::Local::new(&cfg.common.data_wal_dir, false))
}

pub async fn list(prefix: &str) -> object_store::Result<Vec<String>> {
    let files = DEFAULT
        .list(Some(&prefix.into()))
        .map_ok(|meta| meta.location.to_string())
        .try_collect::<Vec<String>>()
        .await
        .expect("Error listing files");
    Ok(files)
}

pub async fn get(file: &str) -> object_store::Result<GetResult> {
    DEFAULT.get(&file.into()).await
}

pub async fn get_range(file: &str, range: Range<usize>) -> object_store::Result<bytes::Bytes> {
    DEFAULT.get_range(&file.into(), range).await
}

pub async fn head(file: &str) -> object_store::Result<ObjectMeta> {
    DEFAULT.head(&file.into()).await
}

pub async fn put(file: &str, data: bytes::Bytes) -> object_store::Result<()> {
    let multi_part_upload_size = get_config().s3.multi_part_upload_size;
    if multi_part_upload_size > 0 && multi_part_upload_size < bytes_size_in_mb(&data) as usize {
        put_multipart(file, data).await?;
    } else {
        DEFAULT.put(&file.into(), data.into()).await?;
    }
    Ok(())
}

pub async fn put_multipart(file: &str, data: bytes::Bytes) -> object_store::Result<()> {
    let path = Path::from(file);
    let upload = DEFAULT.put_multipart(&path).await?;
    let mut write = WriteMultipart::new(upload);
    write.write(data.as_bytes());
    write.finish().await?;
    Ok(())
}

pub async fn del(files: &[&str]) -> object_store::Result<()> {
    if files.is_empty() {
        return Ok(());
    }

    let start = std::time::Instant::now();
    let columns = files[0].split('/').collect::<Vec<&str>>();
    let files = files
        .iter()
        .map(|file| file.to_string())
        .collect::<Vec<_>>();

    if !is_local_disk_storage() && get_config().s3.feature_bulk_delete {
        let files_stream = futures::stream::iter(files)
            .map(|file| Ok(Path::from(file)))
            .boxed();
        match DEFAULT
            .delete_stream(files_stream)
            .try_collect::<Vec<Path>>()
            .await
        {
            Ok(files) => {
                log::debug!("Deleted objects: {:?}", files);
            }
            Err(e) => {
                log::error!("Failed to delete objects: {:?}", e);
            }
        }
    } else {
        let files_stream = futures::stream::iter(files);
        files_stream
            .for_each_concurrent(get_config().limit.cpu_num, |file| async move {
                match DEFAULT.delete(&(file.as_str().into())).await {
                    Ok(_) => {
                        log::debug!("Deleted object: {}", file);
                    }
                    Err(e) => {
                        // TODO: need a better solution for identifying the error
                        if file.ends_with(".result.json") {
                            // ignore search job file deletion error
                            log::debug!("Failed to delete object: {}, error: {:?}", file, e);
                        } else if !is_local_disk_storage() {
                            log::error!("Failed to delete object: {:?}", e);
                        }
                    }
                }
            })
            .await;
    }

    if columns[0] == "files" {
        let time = start.elapsed().as_secs_f64();
        metrics::STORAGE_TIME
            .with_label_values(&[columns[1], columns[2], "del", "remote"])
            .inc_by(time);
    }

    Ok(())
}

pub async fn get_file_meta(file: &str) -> Result<FileMeta, anyhow::Error> {
    let mut file_meta = FileMeta::default();
    let (file_size, parquet_meta) = get_parquet_metadata(file).await?;
    if let Some(metadata) = parquet_meta.file_metadata().key_value_metadata() {
        file_meta = metadata.as_slice().into();
    }
    file_meta.compressed_size = file_size as i64;
    Ok(file_meta)
}

async fn get_parquet_metadata(file: &str) -> Result<(usize, Arc<ParquetMetaData>), anyhow::Error> {
    // get file info
    let info = head(file).await?;
    let file_size = info.size;

    // read metadata len
    let mut data = get_range(file, file_size - parquet::file::FOOTER_SIZE..file_size).await?;
    let mut buf = [0_u8; parquet::file::FOOTER_SIZE];
    data.copy_to_slice(&mut buf);
    let metadata_len = ParquetMetaDataReader::decode_footer(&buf)?;

    // read metadata
    let data = get_range(
        file,
        file_size - parquet::file::FOOTER_SIZE - metadata_len
            ..file_size - parquet::file::FOOTER_SIZE,
    )
    .await?;

    Ok((
        file_size,
        Arc::new(ParquetMetaDataReader::decode_metadata(&data)?),
    ))
}

pub fn format_key(key: &str, with_prefix: bool) -> String {
    let cfg = get_config();
    if !is_local_disk_storage()
        && with_prefix
        && !cfg.s3.bucket_prefix.is_empty()
        && !key.starts_with(&cfg.s3.bucket_prefix)
    {
        format!("{}{}", cfg.s3.bucket_prefix, key)
    } else {
        key.to_string()
    }
}

fn bytes_size_in_mb(b: &bytes::Bytes) -> f64 {
    b.len() as f64 / (1024.0 * 1024.0)
}

#[derive(Debug)]
pub enum Error {
    OutOfRange(String),
    BadRange(String),
}

impl std::fmt::Display for Error {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::OutOfRange(s) => write!(f, "Out of range: {}", s),
            Self::BadRange(s) => write!(f, "Bad range: {}", s),
        }
    }
}

impl From<Error> for object_store::Error {
    fn from(source: Error) -> Self {
        Self::Generic {
            store: "storage",
            source: Box::new(std::io::Error::new(
                std::io::ErrorKind::InvalidInput,
                source.to_string(),
            )),
        }
    }
}

#[derive(Debug)]
pub enum InvalidGetRange {
    StartTooLarge { requested: usize, length: usize },
    Inconsistent { start: usize, end: usize },
}

impl std::fmt::Display for InvalidGetRange {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::StartTooLarge { requested, length } => {
                write!(
                    f,
                    "Start too large: requested {} but length is {}",
                    requested, length
                )
            }
            Self::Inconsistent { start, end } => {
                write!(
                    f,
                    "Inconsistent range: start {} is greater than end {}",
                    start, end
                )
            }
        }
    }
}

pub trait GetRangeExt {
    fn is_valid(&self) -> Result<(), InvalidGetRange>;
    /// Convert to a [`Range`] if valid.
    fn as_range(&self, len: usize) -> Result<Range<usize>, InvalidGetRange>;
}

impl GetRangeExt for GetRange {
    fn is_valid(&self) -> Result<(), InvalidGetRange> {
        match self {
            Self::Bounded(r) if r.end <= r.start => {
                return Err(InvalidGetRange::Inconsistent {
                    start: r.start,
                    end: r.end,
                });
            }
            _ => (),
        };
        Ok(())
    }

    /// Convert to a [`Range`] if valid.
    fn as_range(&self, len: usize) -> Result<Range<usize>, InvalidGetRange> {
        self.is_valid()?;
        match self {
            Self::Bounded(r) => {
                if r.start >= len {
                    Err(InvalidGetRange::StartTooLarge {
                        requested: r.start,
                        length: len,
                    })
                } else if r.end > len {
                    Ok(r.start..len)
                } else {
                    Ok(r.clone())
                }
            }
            Self::Offset(o) => {
                if *o >= len {
                    Err(InvalidGetRange::StartTooLarge {
                        requested: *o,
                        length: len,
                    })
                } else {
                    Ok(*o..len)
                }
            }
            Self::Suffix(n) => Ok(len.saturating_sub(*n)..len),
        }
    }
}
